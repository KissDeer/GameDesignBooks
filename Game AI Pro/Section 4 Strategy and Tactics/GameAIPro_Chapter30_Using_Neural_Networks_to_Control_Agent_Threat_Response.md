# 四杠五、使用神经网络控制智能体威胁响应
**迈克尔·罗宾斯**

## 30.1 引言
神经网络是最古老和最广泛使用的机器学习技术之一，其历史至少可以追溯到20世纪50年代。尽管游戏AI社区中有人担心它们可能不适合游戏，但我们在《最高指挥官2》中使用它们的经验非常积极。如果使用得当，它们可以以比手动编码少得多的努力提供引人注目的行为。在《最高指挥官2》中，神经网络被用于控制AI控制的排对遇到敌人单位时的战斗或逃跑反应，效果显著。神经网络远非无用，它为AI增加了很多价值，而不需要付出过高的努力。

有许多印刷和网络资源描述了神经网络的基础知识，甚至提供了示例代码。《游戏人工智能》[Millington 09]和《游戏编程中的AI技术》[Buckland 02]是入门的好资源，而《游戏编程宝石2》[Manslow 01]提供了示例代码和广泛的实用提示和技巧。本文将重点介绍神经网络在《最高指挥官2》中的具体使用方式。

## 30.2 什么是神经网络
神经网络有许多不同的类型，但本文将重点介绍多层感知器（MLPs），《最高指挥官2》选择它们是因为它们相对容易实现和使用简单。

MLPs通常由三层神经元或“节点”组成，在神经网络文献中通常称为节点。这些层是输入层、隐藏层和输出层。每个节点都有一个与之相关的值，该值在0到1之间，表示其兴奋程度。节点通过单向“权重”连接到其他节点，权重是生物突触的类似物，允许一个节点的兴奋程度影响另一个节点的兴奋程度。在MLP中，每个节点仅从前一层的节点接收刺激，并仅向下一层的节点提供刺激。

数据通过设置输入层节点的兴奋程度输入到MLP中。然后，隐藏层的每个节点接收的刺激量等于内部偏差加上输入层中每个节点的兴奋程度与连接到它的权重的乘积之和。然后，通过对表示其刺激程度的值应用非线性激活函数来计算隐藏层中每个节点的兴奋程度。逻辑函数是MLP的标准激活函数选择，并产生0到1范围内的兴奋程度。

这个过程在网络中的每一层重复，直到网络的输出节点的兴奋程度被更新；这些程度构成了网络的输出，因此也是它对早期输入的响应。MLP的行为完全由其权重和偏差的值决定，训练它的过程包括找到使网络输出与某些理想目标值之间的差异最小化的权重和偏差的值。

## 30.3 设置神经网络
对于《最高指挥官2》，决定使用MLP来控制排对遇到敌人单位时的反应。我们决定总共使用四个MLP，每个排类型一个：陆地、海军、轰炸机和战斗机。我们这样分割MLP，以便每个排类型可以学习它需要的东西，而不会干扰其他排类型。

AI的大部分排逻辑将存在于一个有限状态机中，该状态机将使用MLP来决定排遇到敌人抵抗时该做什么，并将继续使用MLP来重新评估不断变化的情况。MLP提供了一种很好的方式来实现这一点，因为它们可以根据训练快速评估情况。在任何情况下，MLP都可以使AI能够确定它应该首先攻击哪个敌人目标，或者如果发现自己处于劣势则撤退。为了实现这一点，首先需要做的是决定MLP需要哪些信息来做出这些决策以及如何表示这些信息。

### 30.3.1 选择输入
通过设置表示其输入节点兴奋程度的级别的值来向MLP提供输入。这些值通常限制在0到1之间，尽管-1到1的范围也适用于MLP。对于《最高指挥官2》，输入是通过获取某些统计数据的友方和敌方值之间的比率来创建的，这些统计数据包括单位数量、单位健康、每秒总伤害（DPS）、移动速度、资源价值、护盾健康、短程DPS、中程DPS、远程DPS和修复率。所有输入值都被限制在0到1之间，因此比率的倒数也被包括在内，以便在友方统计数据超过敌方统计数据时为网络提供有关统计数据相对大小的有用信息。这些统计数据是从AI排周围半径内的友方和敌方单位收集的。总共计算了17个比率，因此网络有34个输入。

这个相对较大的输入数量在《最高指挥官2》中效果很好，但在其他应用中可能会有问题，特别是如果只有几千个示例可用于训练。这可能导致所谓的“过拟合”，即网络有效地学习了训练数据的某些细节，而不是其中的一般模式。当网络在训练期间的表现明显优于测试时，过拟合就很明显。过拟合最容易通过使用更简单的网络重新训练（当然，也可以通过提供更大的训练数据集）来防止。因此，一般来说，在选择输入时，找到尽可能小的集合是一个好主意。同时，MLP只能考虑您提供给它的信息，因此希望有一个小的输入集需要与包含尽可能多的相关信息的愿望相平衡。最终，您需要进行实验以找到适合您项目的方法。

### 30.3.2 选择输出
当向MLP应用输入时，它以0到1之间的值的形式计算输出，这些值表示其输出节点的兴奋程度。对于《最高指挥官2》，决定每个输出节点将代表排可以采取的行动之一的预期效用。这些行动包括攻击最弱的敌人、攻击最近的敌人、攻击价值最高的敌人、攻击资源生成器、攻击护盾生成器、攻击防御结构、攻击移动单位、攻击工程单位和远程攻击。尽管排可以逃跑，但逃跑的行为与任何单个输出无关。相反，决定如果网络的输出都不超过0.5，排将逃跑，因为这表明没有任何单个行动预计具有特别高的效用。

### 30.3.3 选择隐藏节点的数量
MLP中的隐藏节点负责其学习复杂非线性关系的能力，网络拥有的隐藏节点越多，它可以学习的关系就越复杂。不幸的是，增加隐藏节点的数量也会增加训练时间的成本，并且与增加输入数量一样，会增加过拟合的风险。不幸的是，隐藏节点的最佳数量取决于问题，必须通过试验和错误来确定。一种方法是最初用只有两个或三个隐藏节点测试您的网络，然后添加更多，直到达到可接受的性能。对于更复杂的决策，从更大的网络开始是合理的，但您需要确保经过训练的网络经过彻底测试，以确保其在测试中的性能与训练期间的性能一致。

对于《最高指挥官2》，我们发现具有98个隐藏节点的网络在训练和测试期间都取得了良好且一致的性能。这样的网络对于许多其他应用来说太大了，特别是当训练数据量有限时，但鉴于我们能够生成任意大量的训练数据以及所做决策的复杂性，这对我们来说效果很好。

## 30.4 训练神经网络
训练MLP通常涉及反复迭代一组训练示例，每个示例由输入和目标输出配对组成。对于每对，将输入呈现给网络，网络计算其输出，然后修改网络的权重和偏差，使其输出稍微接近目标输出。这个过程在训练集中的每个示例上重复，每个示例通常在训练过程中呈现数百或数千次。

在《最高指挥官2》中，我们决定不创建固定的训练示例集，而是通过让AI与自己对战来动态生成示例。这是通过在地图上放置两个AI排并让它们像在常规游戏中一样相互战斗来实现的，除了我们会尽可能快地运行游戏以加快迭代时间。在战斗中，AI的排会像在常规游戏中一样行动。AI的神经网络会在敌方排在战场上相遇时决定应该执行哪个行动，通过收集排周围半径内的友方和敌方单位的数据，并将这些数据输入到MLP中。然而，每个排不是实际采取网络建议的行动，而是执行随机行动，并使用适应度函数得出这些行动有多好的度量 - 它们的效用的度量。然后，效用度量形成与随机行动对应的输出节点的目标输出，所有其他输出节点的目标输出设置为每个节点的当前兴奋程度；通过这种方式，网络更新其权重和偏差，以提高其对随机行动效用的估计，但不会尝试更改任何其他输出。使用随机行动而不是网络建议的行动，以确保在各种情况下尝试了良好的行动组合。未经训练的网络通常会在各种情况下重复执行相同的行动，因此学习非常缓慢 - 如果它学习的话。

这个训练过程产生了一个MLP，它通过估计每个不同行动的效用来响应输入。然后，选择最佳行动就是选择与兴奋程度最高的输出相关联的行动的简单问题。确保这些行动适当的关键是确保适应度函数 - 在训练期间评估行动的效用 - 在每种情况下为最合适的行动分配最高的效用。

### 30.4.1 创建适应度函数
适应度函数的工作是评估所选行动的结果，以确定执行该行动后情况变得更好或更糟的程度。对于《最高指挥官2》，这是通过收集用于做出初始决策的相同数据集（单位数量、DPS值、健康等），然后检查采取行动时这些数据值的变化来实现的。

清单30.1给出了我们在《最高指挥官2》中使用的适应度函数的片段。它首先计算每种数据类型的新值和旧值之间的比率。请注意，由于所有这些值可能保持不变或下降，所有这些比率应该在0到1之间，这将后面计算的大小限制在合理的范围内。接下来，我们计算友方单位和敌方单位比率的平均值。这给出了一种感觉，即每一方的总体战术情况不仅在受到的伤害方面，而且在每一个重要能力 - 护盾、伤害输出、剩余单位数量等方面发生了多大的变化。得到的平均值被传递到DetermineNewOutputs中，该函数使用等式30.1确定正确的输出 - 称为期望输出 - 的值应该是什么。

$desiredOutput = output \times(1+( friendRatio - enemyRatio ))$（30.1）

这个期望输出值然后被插入到MLP的相应输出节点中，MLP经历一个调整权重和偏差的过程，从输出层开始，通过反向传播的过程回到输入层。这就是MLP学习的方式。

### 30.4.2 调整学习参数
MLP的训练通常由学习率参数控制，该参数控制网络在调整权重和偏差时所做更改的大小。较高的学习率允许更大的更改，这可以导致更快的学习，但增加了数值不稳定和振荡的风险，因为网络试图接近最佳值；较低的学习率可能使训练变得不切实际地缓慢。因此，一个常见的技巧是开始训练时使用较高的学习率，并随着时间的推移降低它 - 因此您最初获得快速学习，但随着权重和偏差接近其最佳值，调整变得越来越保守。对于《最高指挥官2》，我们最初以0.8的学习率开始，并逐渐将其降低到0.2。

MLP训练算法通常还有一个称为动量的参数，可用于加速学习过程。动量通过在后续调整期间重新应用权重或偏差值的最后一次更改的一部分来实现这一点，从而加速一致的更改并帮助防止快速振荡。与学习率一样，动量参数的较高值最初是好的，因为它加速了学习的早期阶段。对于《最高指挥官2》，我们以0.9的动量值开始，并最终通过将其设置为零完全关闭动量。

### 30.4.3 调试神经网络
神经网络本质上是一个黑盒，这使得调试它们变得困难。您不能只是进去，设置一个断点，并弄清楚它为什么做出它所做的决定。您也不能只是进去并开始调整权重。这是神经网络不太受欢迎的很大一部分原因。一般来说，如果MLP没有按预期执行，那么通常是它接收的输入数据有问题，它的输出解释方式有问题，训练期间使用的适应度函数有问题，或者训练期间暴露的环境有问题。

例如，如果MLP在训练期间表现良好，但在测试期间表现不佳，可能是因为网络在训练期间暴露的环境与测试期间经历的环境不具有代表性。也许单位的组合不同，或者设计中发生了某些变化？这也可能是由于过拟合，在这种情况下，输入较少或隐藏节点较少的网络可能表现更好。如果MLP在训练期间表现良好，但它的行为并不总是合理的，那么可能是训练期间使用的适应度函数有缺陷 - 也许它有时会为不适当的行动分配高效用，或者为适当的行动分配低效用 - 稍后会更多地讨论这一点。如果MLP甚至在训练期间都表现不佳，那么通常是因为它的输入提供的相关信息太少，或者它的隐藏节点太少，无法学习所需的关系。

如果您在游戏中使用神经网络，这些点需要强调。在调试神经网络时，解决方案通常不是通过设置断点来找到失败点。您必须考虑网络的输入、输出以及您的适应度函数如何训练您的神经网络。

### 30.4.4 案例研究：修复适应度函数中的错误
在《最高指挥官2》中，每个玩家开始时都有一个称为ACU的单位，哪个玩家首先摧毁对手的ACU就赢得比赛。然而，当ACU被摧毁时，它会在一次大型核爆炸中爆炸，摧毁大片区域内的大多数较小单位和建筑物。对于神经网络来说，这带来了一个问题：由于网络是在战术交战中训练的，它不知道胜利或失败。它所看到的只是，当它派遣单位攻击ACU时，大多数单位都被摧毁了。

这引入了一个错误，使得AI不愿意派遣部队攻击ACU。它会用大量的单位压倒玩家，但一旦看到ACU，它就会转身逃跑。问题不在行为代码中，也不是通过设置断点可以追踪到的问题；问题在于适应度函数。

一旦我们意识到问题是什么，解决方案就很简单：我们需要修改适应度函数，以考虑敌人ACU的破坏。基本上，我们需要教会神经网络，无论付出什么代价，摧毁ACU都是值得的。这是通过修改适应度函数来实现的，每当敌人ACU被摧毁时，它都会提供一个非常积极的效用度量。适应度函数不再依赖于等式30.1的结果，而是返回一个期望输出，该输出是原始MLP输出的两倍，限制在最大值1.0。用新的适应度函数重新训练网络后，我们看到了巨大的改进。AI如果只有少量单位，就会逃离ACU，但如果它有足够大的群体来击败它，它就会参与战斗，随着敌人的ACU以壮观的方式爆炸，赢得比赛。

## 30.5 调整行为
即使MLP的行为在训练后是固定的，仍然可以使用它来生成表现出各种不同行为的AI。例如，在《最高指挥官2》中，我们为AI个性添加了一个攻击性值。这个值用于修改输入到MLP的比率，以模拟AI的单位比实际更强的效果。这使得MLP高估了更具攻击性行动的效用，产生了一个总体上更具攻击性的AI。

与其总是让AI执行MLP估计效用最高的行动，可以考虑不同的行动选择方案。例如，AI可以随机选择N个最高效用行动之一，或者以与其效用成比例的概率选择一个行动。这两个方案都会产生更多样化的行为，尽管它们都涉及选择可能不是最优的行动，因此可能会产生更容易被击败的AI。

## 30.6 神经网络性能
MLP的运行时性能取决于它拥有的节点数量。在《最高指挥官2》中，每个MLP有34个输入节点、15个输出节点和98个隐藏节点，我们从未见过一个网络计算输出的时间超过0.03毫秒（在八人AI比赛中）。由于向前馈送MLP基本上只是一堆浮点数学运算，这并不奇怪。当然，性能会根据硬件和您的实现细节而有所不同，但查询MLP所花费的时间不太可能成为问题。

## 30.7 使用神经网络的好处
与基于效用的方法等相比，使用神经网络最显著的好处可能是您不必自己想出权重。您不必弄清楚在任何特定决策中健康是否比护盾更重要，或者它们与速度的比较如何。这一切都在训练期间为您解决。《最高指挥官2》的每个神经网络大约需要一个小时的训练才能达到可发布的性能水平。然而，我们确实必须完成训练过程几次，才能最终得到一组运行良好的神经网络，这主要是由于前面提到的ACU问题等障碍。

《最高指挥官2》中使用的输入表示的一个主要好处是，它提供了排组成的抽象表示，即使单个单位的统计数据发生变化，该表示仍然有效；神经网络不关注特定单位，只关注它们的统计数据。只要游戏机制没有任何根本性的变化，随着单个单位的统计数据被修改以产生平衡的游戏，网络能够继续做出良好的决策。

## 30.8 使用神经网络的缺点
像生活中的大多数事情一样，使用神经网络解决方案并非没有代价。与更传统的方法相比，使用它们肯定有一些缺点，其中最重要的是它们的黑盒性质。对于大多数解决方案，您可以想出一个工具，设计师可以使用该工具来调整AI的行为；至少您可以进行小的调整以改变其行为以满足他们的需求。对于神经网络来说，这很困难，如果不是完全不可能的话。在《最高指挥官2》中，我们很幸运，因为我们为战役模式和遭遇战模式使用了单独的AI系统。设计师可以对战役进行任何他们想要的更改，但他们不想控制遭遇战模式。不幸的是，大多数项目都没有那么幸运。

另一个问题是训练时间。与其他技术不同，在其他技术中，您可以轻松地进行小的更改，如果您更改了与神经网络有关的任何事情 - 它的输入、输出的解释、适应度函数和隐藏节点的数量 - 您必须从头开始训练。即使训练是放手的，所花费的时间也使得很难快速尝试事物。

## 30.9 结论
每当提到《最高指挥官2》中神经网络的主题时，经常会问到两个问题：使用它们是否值得，您会再次使用它们吗？两者的答案都是肯定的。我们坚信，如果不使用神经网络，《最高指挥官2》中的AI将不会产生相同的影响。此外，如果有人提议制作《最高指挥官3》，您可以打赌神经网络将发挥作用。

也就是说，神经网络并不适合每个项目，它们当然也不是AI的全部和终结。神经网络与其他任何工具一样，具有特定的优势和劣势。如果您有一组明确的行动或响应，并且设计师不需要太多控制，它们非常方便。但是，如果您的设计师想要微调事物，或者您必须处理多组响应以适应不同的AI个性等情况，那么您可能需要考虑其他选项。
