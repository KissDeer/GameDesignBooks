二杠五、模拟行为树，行为树 / 规划器混合方法
丹尼尔・希尔伯恩
8.1 引言
游戏人工智能必须处理高度的复杂性。设计师通常用复杂的状态图来表示人工智能，这些状态图必须被实现和彻底测试。人工智能智能体存在于一个复杂的游戏世界中，必须有效地查询这个世界并构建关于它的已知模型。必须正确管理动画状态，以便人工智能与世界正确交互。人工智能必须同时提供从完全自主到完全由设计师驱动的一系列控制。
同时，游戏人工智能必须具有灵活性。设计师快速且频繁地改变人工智能结构。人工智能的实现严重依赖于其他人工智能和游戏系统的实现，这些也经常发生变化。对这些外部系统所做的任何假设都很容易变得无效 —— 通常几乎没有警告。游戏人工智能还必须与一系列玩家风格进行适当交互，让许多玩家能够愉快地玩你的游戏。
人工智能程序员有许多技术可用来帮助解决这些问题。与任何技术一样，所有技术都有其优点和缺点。在接下来的几个部分中，我们将简要概述其中的两种技术：行为树和规划器。我们将简要概述它们试图解决的问题以及它们难以解决的问题。然后，我们将讨论一种混合实现，它借鉴了这两种方法的优点。
8.1.1 行为树
游戏人工智能中的行为树用于建模智能体可以执行的行为。树结构允许将基本动作（例如，跳跃、踢）组合起来创建更高层次的行为（例如，飞踢）。这个更高层次的行为可以被视为基本行为，并用于组合更高层次的行为（例如，攻击）。行为树还包括约束的概念，它可以附加到树中任何级别的行为上，以防止在世界状态与约束所需的状态不匹配时选择该行为（图 8.1）。
行为树非常擅长建模人工智能可以做什么。它们允许设计师采用非常低级的动作并将它们组合起来，以创建设计师希望人工智能可用的精确的高级动作集。树结构可以轻松地对设计师抛出的任何复杂设计进行建模。方便的是，这种结构也非常类似于设计师经常用来描述人工智能的图表，这使得设计师和程序员在讨论人工智能实现时可以使用相同的语言。树结构也很容易配置，特别是如果有图形设计工具可用。这允许设计师快速迭代和改进人工智能设计。
不幸的是，行为树不太擅长指定人工智能应该做什么。为了让树知道应该执行哪个动作，它必须对世界状态有深入的了解，包括其他人工智能或游戏系统是如何实现的。它还必须知道它的每个行为如何影响 —— 以及如何被 —— 世界状态的变化所影响。这导致了对其他可能发生变化的系统的依赖网络。如果这些系统中的任何一个发生变化，您将不得不相应地更新您的行为树。这种设计比我们希望的要脆弱得多。更可取的是，即使其他系统或其自身的内部结构发生变化，行为树也能在不进行任何修改的情况下正常工作。稍后，我们将讨论通过从人工智能规划器中获取一些线索来解决这些问题的方法。
8.1.2 规划器
游戏人工智能中的规划器用于根据当前世界状态创建一系列基本动作来实现某个目标。这个动作序列称为计划。规划器维护一个世界状态模型、一个人工智能可用的所有基本动作的集合以及一个目标启发式算法。世界状态模型包含启发式算法所需的关于世界的任何信息。例如，世界状态可能包括可用敌人的列表及其健康值。规划器了解每个动作如何影响世界状态，并且由于计划只是这些动作的序列，规划器也了解任何计划如何影响世界状态。例如，如果敌人靠近，踢动作会对敌人造成一些伤害，而跳跃动作会使人工智能更接近敌人。
目标启发式算法根据计划实现启发式目标的程度对给定计划进行评分。在我们的示例中，战斗启发式算法会给导致敌人受损的计划打高分。因此，如果人工智能靠近敌人，高分计划可能只包括踢动作。但是，如果人工智能离敌人太远，踢动作无法造成伤害，该计划将获得低分。但是，如果我们在踢之前插入一个跳跃动作，现在人工智能可以移动并攻击敌人，这个计划将获得高分。有了所有这些部分，规划器可以动态地创建实现高级目标的计划，无论当前世界状态如何（图 8.2）。
如您所见，规划器非常擅长管理人工智能应该做什么。它们允许设计师通过在规划器的启发式算法中评估世界状态来为人工智能指定高级目标，而不是尝试为特定情况设计特定行为。规划器能够做到这一点，是因为它在人工智能所做的事情（动作）和人工智能应该做的事情（启发式算法）之间保持了非常严格的分离。这也使人工智能在面对设计更改时更加灵活和持久。如果因为团队没有时间完善动画而删除跳跃动作，只需将其删除。人工智能仍将为其当前世界状态创建最佳计划。如果踢动作突然也会发出冲击波，您只需将其添加到踢动作的结果描述中。您不需要仅仅因为更改了人工智能的行为就更改它应该做的事情。
虽然规划器的灵活性是一个很大的优势，但它也可能是一个很大的劣势。通常，设计师希望对人工智能可以执行的动作序列有更多的控制。虽然您的人工智能能够自行创建飞踢计划很酷，但它也可能创建一个连续 27 次跳跃的序列。这打破了我们的人工智能应该产生的智能幻觉，这显然不是我们想要发生的事情。有一些技术可以防止这种不受欢迎的计划，但很难预测规划器可能崩溃的所有情况。可以理解，这导致许多设计师对规划器不满，因为他们通常更喜欢对角色的行为有更多的控制。
这是人工智能设计师和程序员必须不断处理的经典权衡：行为树提供的完全设计（脆弱）的人工智能与规划器提供的完全自主（不可预测）的人工智能之间的选择。虽然这场战斗已经持续了一段时间，但幸运的是，这不是一个二元选择。这两种方法之间的空间是最佳解决方案所在。有许多行为树和规划器的实现以及许多其他技术试图解决这个问题，其中之一是我们的下一个主题。我将快速描述这种方法的工作原理，然后我们将深入探讨我如何在最近的一个项目中实现它。
8.1.3 行为树 / 规划器混合
混合方法的基本前提很简单：结合行为树和规划器的优势，产生一个在面对设计更改时灵活且持久的人工智能系统，同时允许设计师完全控制人工智能可用的动作结构。它使用世界状态模型和启发式算法，就像规划器一样。然而，规划器动态构建基本动作序列并使用其启发式算法选择最佳序列，而混合方法使用其启发式算法在预制行为树的分支之间进行选择。
使用行为树允许设计师完全控制可用的动作，并在迭代时轻松重新设计这些动作的结构。然而，正如我们之前提到的，行为树通常对设计更改非常抗拒，因为更改动作的内部结构必须反映在树中的父选择器节点中。这就是我们的规划器部分来拯救的地方。
请记住，我们的方法还包括规划器的世界状态模型和启发式算法。我们通过为每种节点类型实现模拟步骤将这些纳入行为树。对于叶节点，这个模拟步骤就像规划器系统中的基本动作一样，简单地返回一个结果世界状态。然而，行为树的递归结构也允许我们递归地模拟复合行为。序列节点按顺序模拟其每个子行为，然后返回累积结果。选择器节点模拟其每个子行为，以确定如果选择该节点，结果世界状态会是什么。然后，这些结果通过启发式函数进行处理，为每个子行为生成一个分数。然后，选择器节点使用这些分数来确定选择哪个节点，并返回该节点的结果作为自己的结果。
这种设计允许我们构建对其内部结构一无所知的行为树。我们可以创建一个攻击选择器节点，它由任意数量的攻击行为组成，它将选择最适合当前世界状态的行为，而无需知道龙拳或飞踢或它们最适合的时间。任何选择器节点只需要模拟它的每个子节点，并选择具有最高启发式分数的节点。这允许我们更改树的内部结构，而无需更改层次结构中更高的任何代码。它还允许我们更改叶动作对世界的影响，而无需担心更新整个树以补偿更改的设计。这就是我们想要的。行为树结构允许设计师完全控制人工智能可以做什么，而规划器机制负责确定人工智能应该做什么。
8.2 《Kinect 星球大战》中的绝地人工智能
我们在为 Terminal Reality Inc. 的《Kinect 星球大战》开发绝地人工智能时开发了这种行为树 / 规划器混合方法，该公司非常友好地提供了本文的源代码作为示例材料。我们在本文中只能介绍该系统的一个子集，但本书的网站（[http://www.gameaipro.com](http://www.gameaipro.com/)）提供了整个系统的示例代码。专有引擎的内容被省略了，但如果您有兴趣查看，所有绝地人工智能的部分都在那里。废话不多说，让我们来创建一个绝地武士！
8.2.1 绝地记忆
我们的绝地武士首先需要的是对世界的内部了解。这是我们之前讨论的世界状态模型，我们称之为绝地人工智能记忆（见清单 8.1）。它封装了我们的动作可以在世界上操纵的一切，包括绝地武士、绝地武士当前的受害者、任何附近的敌人和任何传入的威胁。它还提供了一个 simulate () 方法，允许任何动作更新随时间变化的内存部分（例如，位置），以及一个 simulateDamage () 方法，允许任何行为轻松模拟对给定敌人造成的伤害。
8.2.2 绝地行为树
现在我们有了世界状态表示，让我们来看看绝地武士的行为树实现。我们所有的行为树节点，我们称之为 Actions，提供了标准的 begin/update/end 行为树节点接口。这些节点从它们的 begin 和 update 操作向其父节点返回一个 EJediAiActionResult 值，以让父节点知道 Action 的当前状态（见清单 8.2）。
Actions 还提供了一个 checkConstraints () 方法，它遍历附加的 Constraint 对象列表（见清单 8.3）。这个方法也可以被重写，以允许 Action 子类检查特定于这些子类的 Constraints。Constraint 对象提供了在动作进行中或模拟时跳过约束的选项，这为约束子类提供了一些稳定性。例如，让我们考虑附加到龙拳序列的距离约束，以防止我们的人工智能在敌人太远时执行它。如果我们开始序列并且敌人移动得足够远导致约束失败，人工智能将立即退出序列，这可能不是我们想要的。对于人工智能来说，继续执行序列并简单地错过敌人可能更可取。如果我们设置约束在动作进行中被跳过，这正是会发生的事情。
最后，Actions 提供了我们的模拟接口。每个 Action 包含一个模拟摘要对象，它封装了我们的启发式函数关心的一切。这个摘要还包含一个 EJediAiActionSimResult 值，它由启发式算法计算，并指定了动作的可取性。最初，我们使用 0 到 1 之间的浮点数来指定这个值，但这样很难从启发式算法中获得稳定、可预测的结果。我们将结果简化为清单 8.4 中的值。
现在我们已经指定了 Action 的所有部分，我们可以在清单 8.5 中的 CJediAiAction 类中将它们全部组合在一起。它提供了标准的 begin/update/end 接口、模拟接口和 Constraint 接口。
8.2.3 绝地模拟
现在我们已经定义了行为树组件，让我们来看看规划器方面的事情：模拟。当我们开始模拟一个 Action 时，我们创建当前世界状态的摘要。然后，我们以与模拟 Action 实际执行时相同的方式修改世界状态。例如，当模拟 SwingSaber Action 时，我们对受害者造成伤害，并将世界状态模拟向前推进与挥舞光剑所需的时间相同。模拟完成后，我们创建结果世界状态的摘要，并计算与初始状态摘要相比，这个新状态的可取性（见清单 8.9）。这个最终摘要将被传递回父 Action，并将在行为树从其他 Action 集合中选择此 Action 时使用。
这个系统的真正核心是规划器启发式算法，我们在这里计算模拟结果（见清单 8.10）。这个函数代表了我们人工智能的当前目标。在这种情况下，绝地武士的唯一目标是避免伤害和威胁，同时对受害者造成伤害。启发式算法通过将 Action 模拟后的世界状态分类为 EJediAiActionSimResult 值之一（不可能、有害、无关、有益等）来实现这一点。
现在我们已经定义了如何计算我们人工智能的模拟结果，让我们来看看它如何适应实际的模拟步骤：SwingSaber Action（见清单 8.11）。
8.3 现在让我们来一些难题……
游戏开发是一个迭代的过程，从构思到最终产品，您的系统会发生很多次变化。即使您的系统没有被重新设计，其他系统的设计更改也可能会改变您的实现行为。因此，我们的系统必须很好地处理这些变化。正如我们之前讨论的，我们混合系统的全部要点是提供灵活性，以尽可能少的更改来处理这些变化。那么让我们通过查看《Kinect 星球大战》中的一些设计更改来看看它的表现如何。
8.3.1 绝地技能水平
《Kinect 星球大战》有三种不同类型的绝地武士：绝地大师、低级学徒绝地武士和第二名玩家的绝地武士。最初，这些都是使用相同的设计实现的。后来，设计团队添加了一个警告，即每个绝地武士应该有一个技能水平来指定他在战斗中的能力。这将使像马夫拉・赞恩这样的绝地大师在战斗中比你的绝地伙伴或游戏中的其他学徒绝地武士更有能力。
我们通过让技能水平指定绝地武士击败每种敌人类型的速度来实现这一点。这使得马夫拉能够快速消灭敌人，而学徒绝地武士则需要更长的时间。为了实现这一点，我们在世界状态中添加了一个 victimTimer 成员来跟踪我们获得当前受害者以来经过的时间。然后，我们在启发式算法中添加了一个语句，以阻止在当前技能水平指定的计时器过期之前杀死受害者（见清单 8.10）。
就是这样。我们不需要更改任何行为树 Actions 或模拟代码。启发式算法已经知道给定的动作是否会杀死受害者，因为我们是在模拟每个动作，而不是将选择逻辑硬编码到选择器中。所以规划器履行了它的承诺，允许我们更改目标而无需修改任何 Actions。
8.3.2 绝地错误
出现的另一个问题是错误的想法。绝地武士总是击败他们的敌人是不现实的；他们有时应该失败。此外，设计师希望绝地人工智能展示针对各种敌人类型不该做的事情。然而，我们的整个系统是建立在绝地武士会选择最佳选项的想法之上的。我们可以制作一个自定义选择器，选择最差的选项而不是最佳选项，但它仍然会向其父节点返回负模拟结果，然后父节点不会选择它来运行。
起初，这似乎是系统的一个缺陷，直到我们思考什么定义了一个 “错误”。显然，绝地武士总是会尝试选择可用的最佳 Action。但是，如果他们计算错误并选择了一个实际上有害的 Action 呢？这将正确地反馈到行为树中，然后有害的 Action 将被选择。为了创建这种计算错误，我们需要在任何 Action 的模拟步骤中插入不正确的信息。我们没有为每个 Action 添加这些特殊情况，而是添加了一个特殊的 Action 称为 FakeSim。FakeSim Action 是一种特殊类型的复合 Action，称为装饰器，它包装另一个 Action 以向其添加额外的功能。FakeSim 负责通过直接修改世界状态来向包装的 Action 的模拟步骤添加不正确的信息。例如，有些敌人有一个盾牌，使他们对光剑攻击免疫。如果我们想让绝地武士攻击敌人来展示敌人在盾牌升起时是无敌的，我们可以用 FakeSim 装饰器包装 SwingSaber Action，在模拟步骤中降低受害者的盾牌。然后，SwingSaber 模拟会认为绝地武士可以伤害敌人并给它一个良好的模拟结果。这将允许选择 SwingSaber，即使它实际上不会有益。
这最终成为处理此设计要求的好方法。它允许我们在系统中的任何地方插入特定的错误，而无需修改任何 Action 类。它还允许我们避免编写特殊情况代码来处理插入这些错误。我们只是向系统中插入一些不正确的领域知识，这反映了人们在现实生活中犯错误的方式。所以行为树履行了它的承诺，允许我们轻松设计规划器无法自行处理的非常特定的 Action 序列。
8.4 结论
我们讨论了行为树和规划器的一些优点和缺点。行为树非常擅长允许设计师精确地定义人工智能可以做什么，而规划器非常擅长允许设计师轻松指定人工智能应该做什么。我们还讨论了如何利用混合方法来实现这两种方法的优点。最后，我们看了这个系统如何在《Kinect 星球大战》中用于创建绝地人工智能。这种方法为设计师提供了行为树的所有控制以及规划器的所有耐久性和灵活性，允许它顺利处理设计更改，并且对代码的更改很少。这才是真正的重点。