二杠六、效用理论简介
大卫・“雷兹”・格雷厄姆
9.1 引言
决策制定是任何人工智能系统的核心。有许多不同的决策制定方法，本书的其他章节讨论了其中的几种。我们遇到的最强大和最有力的系统之一是基于效用的系统。基于效用的系统的一般概念是，同时对每个可能的行动进行评分，并选择得分最高的行动之一。就其本身而言，这是一种非常简单和直接的方法。在本文中，我们将讨论常见技术、最佳实践、避免的陷阱，以及如何将效用理论最佳应用于您的人工智能。
9.2 效用
效用理论是一个在游戏甚至计算机出现之前就已经存在的概念。它已被用于博弈论、经济学和许多其他领域。效用理论背后的核心思想是，给定模型内的每个可能的行动或状态都可以用一个单一的、统一的值来描述。这个值，通常称为效用，描述了该行动在给定上下文中的有用性。例如，假设您需要为您的猫购买一个新玩具；所以您上网并找到了一个完美的玩具。一个网站以 4.99 美元的价格出售，而另一个网站以 2.99 美元的价格出售完全相同的玩具。假设交付时间相同，您可能会选择 2.99 美元的玩具。在这种情况下，该选项通常比 4.99 美元的玩具具有更高的效用，因为最终您会留下更多的钱。
当您需要比较两个不可直接比较的事物的价值时，这个过程会变得更加困难。例如，在前面的例子中，假设两个网站的交付时间不同。4.99 美元的玩具将在两天后到达您的家，而 2.99 美元的玩具将在五天后到达。在这种情况下，选择不再是简单地比较两个价格值的问题。为了衡量行动的总体价值，必须在时间和金钱之间进行某种转换。我们可以说每天价值 1 美元，这意味着 4.99 美元玩具的总成本是 6.99 美元，而 2.99 美元玩具的总成本是 7.99 美元。在这种情况下，4.99 美元的玩具是赢家（因为它让您在金钱 + 时间的综合成本上更低）。您还可以权衡其他因素，如网站忠诚度、朋友的推荐、历史、客户评价以及您可能认为相关的任何其他因素。所有这些因素都有自己的效用分数，您可以将它们组合起来，为决策创建总预期效用。
值得注意的是，效用与价值不同。价值是一个可测量的量（如上面的价格）。效用衡量我们对某事物的渴望程度。这可能会根据个性或情况的背景而改变。如果您是亿万富翁，您可能会选择 4.99 美元的猫玩具，因为您可能更看重时间而不是金钱。您节省的 2 美元是微不足道的。另一方面，如果您非常贫穷，您可能会选择更便宜的玩具并等待额外的时间，因为那额外的 2 美元对您来说真的很重要。这笔钱具有完全相同的价值，但根据其被考虑的背景，金钱的效用是可变的。这可能会随时改变。现在，随身携带绷带的效用可能很小。如果您不小心割伤自己，拥有绷带的效用就会上升。
9.2.1 一致的效用分数
在计算效用分数时，保持一致非常重要。因为效用分数是相互比较以做出最终决策的，所以它们在整个系统中必须在相同的尺度上。正如您将在本文后面看到的，分数通常以有意义的方式组合以产生其他分数。因此，使用归一化分数（从 0 到 1 的值）提供了一个合理的起点。归一化分数通过平均很容易组合，可以在给定任何数字范围内的任何值时轻松计算，并且由于它们在相同的尺度上，因此很容易比较。值得注意的是，任何值范围都可以工作，只要不同变量之间具有一致性。如果一个人工智能智能体对一个行动的评分为 15，您应该立即知道这在整个系统的背景下意味着什么。例如，这个 15 是指 25 分中的 15 分还是 15%？
9.3 最大期望效用原则
使用基于效用的人工智能进行决策的关键是计算人工智能智能体可以采取的每个行动的效用分数（有时称为权重），然后选择得分最高的行动。当然，大多数游戏世界都是不确定的，因此通常不可能计算出确切的效用。如果您无法确定执行该行动的结果，就很难知道该行动是否更可取。这是效用理论的核心，也是它最有用的地方。例如，如果我们有处理能力来计算国际象棋游戏的整个游戏树，那么对移动进行评分就没有必要了 —— 我们只需确定移动序列是否导致胜利、失败或平局。我们目前没有这种能力，因此我们根据我们认为移动的强度来对每个移动进行评分。只要有合理的评分系统，基于效用的人工智能非常擅长根据不完整的信息做出 “最佳猜测”。
最常见的技术是将效用分数乘以每个可能结果的概率，并将这些加权分数相加。这将为您提供行动的预期效用。这可以用数学公式 9.1 表示。

在这种情况下，D 是对该结果的渴望（即效用），P 是该结果发生的概率。这个概率是归一化的，使得所有概率的总和为 1。这适用于可以选择的每个可能的行动，并且选择具有最高预期效用的行动。这被称为最大期望效用原则 [Russell et al. 09]。
例如，在一个角色扮演游戏中，攻击玩家的敌人人工智能有两个可能的结果 —— 要么人工智能击中玩家，要么错过。如果人工智能有 85% 的机会击中玩家，并且成功击中玩家的计算效用分数为 0.6，则调整后的效用将为。（请注意，在这种情况下，错过玩家的效用为零，因此无需考虑在内。）进一步来说，如果将此攻击与使用不同武器的攻击进行比较，例如，有 60% 的机会击中，但如果成功，效用分数为 0.9，则调整后的效用将为。尽管击中的机会较小，但第二种选择提供了更大的总体预期效用。
9.4 决策因素
很少有任何给定的决策仅依赖于单个数据点。决策因素可以被视为决策的单个考虑点。例如，当决定从哪个网站购买猫玩具时，我们通常不仅仅考虑价格，还考虑品牌忠诚度、运输时间、客户评价等。这些数据点中的每一个都是我们在计算是否从该网站购买的最终效用分数时权衡的因素。因素也可以通过权重进一步修改，权重决定了人工智能对该特定因素的关注程度，从而模拟个性。
实现这一结果的一种方法是将公式 9.1 中的预期效用计算应用于每个决策因素，以得出该因素的效用。假设这些分数都是归一化的，您可以将它们平均在一起，得出该决策的最终效用分数。这使我们可以将决策定义为只不过是决策因素的某种组合。
这个原则最好用一个例子来说明。假设我们有一个蚂蚁模拟游戏，人工智能必须决定是扩大蚁群还是繁殖。对于这些决策，我们要考虑三个不同的因素。第一个是蚁群的总体拥挤程度。如果蚂蚁太多，我们需要扩大以容纳更多。第二个是蚁群的健康状况，我们将其定义为基于食物储存的充足程度。蚂蚁卵需要保持在特定温度下；因此有专门建造的托儿所来容纳卵并照顾它们。这些托儿所的空间量是第三个决策因素。这些决策因素基于游戏统计数据，这些数据确定每个因素的分数。人口和最大人口决定了蚁群中有多少蚂蚁以及根据当前蚁群大小可以存在的蚂蚁数量。食物统计数据表示食物储存的充足程度，测量值为 0 到 100。托儿所空间统计数据也从 0 到 100 测量，表示托儿所中有多少空间。您可以将最后这些统计数据视为百分比。
然后，将决策因素的分数组合起来，形成两个行动的最终分数。在这种情况下，拥挤程度和健康状况平均在一起形成扩展行动的分数，而托儿所空间和健康状况平均在一起得到繁殖行动的分数。图 9.1 显示了这种组合。
通过将归一化分数平均在一起，我们可以构建一个无尽的组合链。这是一个非常强大的概念。每个决策因素实际上都与其他决策因素隔离开来。我们唯一知道或关心的是，输出将是一个归一化分数。我们可以轻松添加更多的游戏统计输入，例如到敌方蚁群的距离。这可以输入到一个决策因素中，用于决定繁殖哪种蚂蚁。您也可以轻松地移动决策因素，以不同的方式组合它们。如果您希望拥挤程度对繁殖决策产生负面影响，您可以从 1.0 中减去拥挤程度，并将其平均到繁殖的分数中。
这种技术最强大的用途之一是构建一个工具，允许您直接操作决策因素和游戏状态数据。这个工具将允许设计师拖放框并用箭头连接它们，非常类似于图 9.1 的布局。每个箭头将以不同的方式组合决策因素。例如，您可以将一些决策因素平均在一起，将其他决策因素相乘，选择另一组的最大值或最小值等。某些决策因素也可以被赋予权重，使这些因素更重要或更不重要。几乎有无限的可能性。
9.5 计算效用
到目前为止，我们已经看到了如何在给定一组结果的情况下计算效用，以及如何结合多个决策因素的效用来得出决策的最终效用。下一步是获取任意游戏值并将其转换为效用分数。计算决策因素的初始效用是高度主观的；两个不同的程序员将编写两个不同的效用函数，即使给定相同的输入，也会产生不同的输出。在上面的蚂蚁示例中，我们选择通过将当前食物量除以最大食物量来将健康表示为线性比率。这可能不是一个非常现实的计算，因为当商店大部分都满时，蚁群不应该关心食物。我们想要的是某种二次曲线。
效用理论的关键是理解输入和输出之间的关系，并能够描述由此产生的曲线 [Mark 09]。这可以被视为一个转换过程，您将一个或多个来自游戏的值转换为效用。想出合适的函数实际上更多的是艺术而不是科学，通常这是您将花费大部分时间的地方。有大量不同的公式可以用来生成合理的效用曲线，但其中一些经常出现，值得进行一些讨论。
9.5.1 线性
线性曲线形成一条具有恒定斜率的直线。效用值只是输入的乘数。公式 9.2 显示了计算给定值的归一化效用分数的公式，图 9.2 显示了结果曲线。

在公式 9.2 中，x 是输入值，m 是该输入的最大值。这实际上只是一个归一化函数，对于线性输出来说，这就是我们所需要的。
9.5.2 二次
二次函数形成一条抛物线曲线，使其开始缓慢，然后向上弯曲非常快。实现这一点的最简单方法是在公式 9.2 中添加一个指数。公式 9.3 显示了一个示例。

随着 k 的值增加，曲线的陡峭程度也会增加。由于公式对输出进行了归一化，它总是会收敛于 0 和 1，因此 k 的大值对 x 的低值影响很小。图 9.3 显示了 k 的三个不同值的曲线。
也可以旋转曲线，使得对于 x 的低值而不是高值，效果更紧迫。如果您使用 0 到 1 之间的指数，曲线实际上会旋转，如图 9.4 所示。
9.5.3 逻辑函数
逻辑函数是创建效用曲线的另一个常见公式。它是几种 S 形函数之一，在输入范围的中心放置最大的变化率，在两端接近 0 和 1 时逐渐减小。逻辑函数的输入范围可以是任何东西，但实际上有效地限制在 [–10…10]。生成比这更大的曲线真的没有多大意义，而且范围通常会进一步限制。例如，当 x 为 6 时，EU 将为 0.9975。
公式 9.4 显示了逻辑函数的公式，图 9.5 显示了结果曲线。请注意常数 e 的使用。这是欧拉数 —— 自然对数的底数 —— 大约是 2.718281828。这个值可以调整以影响曲线的形状。随着数字的增加，曲线将变得更加尖锐，开始类似于方波。随着数字的减小，它将变软。

9.5.4 分段线性曲线
到目前为止，我们列出的曲线绝不是完整的列表。您可以使用许多不同的方法将输入转换为其他东西。有时，拥有一个数学公式是不够的。设计师通常需要微调各种给定输入的特定输出。
例如，考虑所有《模拟人生》游戏面临的一个问题，即让模拟人生在饥饿时进食。所有模拟人生都有一个饥饿统计数据，用于衡量他们的饱腹感。这个饥饿统计数据越低，模拟人生就越饥饿。一个天真的评分实现可能是使用如图 9.4 所示的旋转二次曲线来模拟饥饿，或者使用 k 值较小的曲线。这将使模拟人生在饥饿统计数据较低时变得非常饥饿。问题是，即使他们的饥饿统计数据大部分都已满，他们仍然有可能选择进食。机会很小，但最终会被选中。设计师想要更精细的控制。他们希望模拟人生能够完全忽略饥饿，直到达到一个阈值，然后变得有点饿，然后突然变得非常饿。没有办法用一个简单的数学公式来构建和调整这个特定的曲线，所以解决方案是创建一个自定义曲线。自定义曲线的一个例子是分段线性曲线。
分段线性曲线只是一个定制的曲线。这个想法是，您手动调整一堆代表您想要的阈值的 2D 点。当曲线被要求根据给定的 x 值给出 y 值时，它会找到最接近该 x 值的两个点，并在它们之间进行线性插值以得出答案。这允许您创建任何形状的曲线，这正是《模拟人生》所使用的。图 9.6 显示了一个可能用于饥饿的简单响应曲线。
还有许多其他类型的自定义曲线。例如，图 9.6 中的曲线可以更改，使得 15 到 60 的值使用二次曲线计算，而其余的值使用线性计算。您可以拥有的组合数量没有限制。
9.6 选择行动
一旦为每个行动计算了效用，下一步就是选择其中一个行动。有许多方法可以做到这一点。最简单的方法是选择得分最高的选项。对于一些游戏，这可能正是您想要的。国际象棋人工智能绝对应该选择得分最高的移动。策略游戏可能也会这样做。
对于一些游戏（如《模拟人生》），选择绝对最佳的行动可能会让人感觉非常机械，因为在这种情况下，该行动很可能总是被选中。另一种解决方案是将效用分数用作权重，并根据权重随机选择其中一个行动。这可以通过将每个分数除以所有分数的总和来获得该行动将被选中的百分比机会来实现。然后您生成一个随机数，并选择该数字对应的行动。然而，这往往会产生相反的问题。您的人工智能智能体在大多数时候会表现得相当好，但时不时地，他们会选择一些完全愚蠢的事情。
您可以通过从得分最高的行动中选择一个子集，并使用加权随机选择其中之一来获得两全其美的结果。这可以是一个调整的值，例如从得分最高的五个行动中选择，也可以是基于百分位数的，您选择最高得分，同时考虑得分在例如 10% 以内的行动。
这通常会解决手头的问题，但也可能会有一些时候，某些行动集完全不合适。您甚至可能不想对它们进行评分。例如，假设您正在制作一个第一人称射击游戏，并且有一个守卫人工智能。您可能有一些他要考虑的行动集，例如喝咖啡、与他的同伴守卫聊天、检查绒毛等。如果玩家向他开枪，他甚至不应该考虑这些行动，而只尝试对涉及战斗的行动进行评分。在《模拟人生》的一个类似例子中，如果一个模拟人生饿死了，她不应该费心对导致她满足娱乐动机的行动进行评分。
解决这个问题的最直接方法是使用分桶，也称为双重效用人工智能 [Dill 11]。所有行动都被分类到桶中，每个桶都被赋予一个权重。更高优先级的桶总是首先被处理。如果这些更高优先级的桶中有任何有效的行动，它们总是在低优先级桶中的行动之前被选中。在上面的第一人称射击游戏示例中，所有战斗行动都将在比空闲行动更高优先级的桶中。如果有任何有效的战斗行动可采取（即，它们的得分高于 0），守卫将始终选择其中之一，并且不会考虑任何空闲行动。只有当所有战斗行动都无效时，守卫才会选择一个空闲行动。
桶也可以根据情况改变优先级。在《模拟人生》中，动机根据它们的效用值被分桶。得分最高的动机被分组到一个桶中，而不会考虑下面的任何动机。一旦分桶完成，模拟人生将对每个对象上的各个行动进行评分，但只对解决这些分桶动机的行动进行评分。因此，一个饥饿的模拟人生永远不会考虑看电视，除非他们找不到任何可以解决饥饿的东西。这个概念如图 9.7 所示。
在图 9.7 中，您可以看到有两个桶，一个用于饥饿，一个用于娱乐。饥饿得分为 0.8，娱乐得分为 0.4。模拟人生将遍历饥饿桶中的所有可能行动，并假设其中任何行动都是有效的，将选择一个。模拟人生不会考虑娱乐桶中的任何事情，即使其中一些行动的得分更高。这是因为饥饿比娱乐更紧迫。当然，如果饥饿桶中的所有行动都无效，模拟人生将转向下一个得分最高的桶。桶本身的得分基于设计师创建的响应曲线。这导致模拟人生总是试图解决最紧迫的欲望，并选择最佳行动之一来解决该欲望。
9.7 惯性
在任何人工智能系统中都值得提出的一个问题是惯性的概念。如果您的人工智能智能体试图在每一帧都做出决定，那么可能会遇到振荡问题，特别是如果您有两个得分相似的事情。例如，假设您有一个第一人称射击游戏，人工智能意识到它处于一个糟糕的位置。敌人士兵开始对 “攻击玩家” 和 “逃跑” 的评分均为 0.5。如果人工智能每帧都做出新的决定，那么他们可能会开始显得非常疯狂。人工智能可能会向玩家射击几次，开始逃跑，然后再次射击，然后重复。这种行为的振荡看起来非常糟糕。
一种解决方案是为您当前正在进行的任何行动添加权重。这将导致人工智能倾向于保持承诺，直到真正更好的事情出现。另一种解决方案是使用冷却时间。一旦人工智能智能体做出决定，它们就会进入冷却阶段，在此阶段，留在该行动中的权重非常高。此权重可以在冷却期结束时恢复，也可以逐渐降低。
另一种解决方案是延迟做出另一个决定 —— 要么在一段时间内，要么直到当前行动完成。然而，这实际上取决于您正在制作的游戏类型以及您的决策 / 行动过程的工作方式。在《模拟人生：中世纪》中，模拟人生只会在其交互队列为空时尝试做出决定。一旦他们选择了一个行动，他们就会致力于执行该行动。一旦模拟人生完成（或未能完成）他们的行动，他们就会选择一个新的行动。
9.8 演示
本书网站（[gameaipro.com](https://gameaipro.com/)）上的演示展示了本文中的许多概念。它是一个简单的基于文本的战斗程序，类似于 80 年代基于菜单的战斗角色扮演游戏，如《勇者斗恶龙》（又名《龙与地下城》）和《最终幻想》。您与一个怪物进行回合制决斗，您和怪物都有能力攻击对方、使用治疗药水治疗或逃跑。攻击会造成随机数量的伤害，治疗会使用三个治疗药水中的一个来治疗随机数量的生命值，逃跑有 50% 的机会成功逃跑。相关代码在 AiActor.cpp 中，其中包含所有评分函数，并负责在轮到人工智能智能体行动时选择行动。关键函数是 ChooseNextAction ()，它接受对手智能体并返回要执行的行动。这个函数调用每个评分函数来计算它们的分数，并使用加权随机选择一个。
9.8.1 决策因素
在做出决策时，人工智能考虑四个基本因素。第一个是攻击欲望，它基于一个调整值，该值随着在一次攻击中杀死玩家的可能性增加而线性增加。这会导致智能体在游戏后期变得更加激进并承担更多风险，如公式 9.5 所示。这是一个有界线性曲线的好例子。公式中 a 的值是智能体的调整攻击性，这是默认分数。

图 9.8 显示了公式 9.5 中 a 设置为 0.6 时的结果曲线。攻击欲望曲线。
第二个决策因素是威胁。这是一个曲线，用于测量如果玩家造成最大伤害，智能体当前生命值将被夺走的百分比。它的形状类似于二次曲线，由公式 9.6 生成。

图 9.9 显示了威胁的结果曲线。
第三个决策因素是智能体对健康的渴望。这使用了公式 9.4 中逻辑函数的变体。随着智能体生命值的减少，其治疗的欲望会增加。公式 9.7 显示了这个决策因素的公式。

结果曲线是一个漂亮、平滑的 S 形曲线，如图 9.10 所示。请注意指数中添加了 + 6。这就是将曲线推到正 x 轴而不是围绕 0 居中的原因。
9.8.2 决策制定和游戏玩法
演示是您和一个凶猛的效用曲线之间的回合制决斗。您首先选择一个可用的行动，然后您的对手决定如何响应。这将继续进行，直到您中的一个死亡或逃跑。人工智能对手首先通过运行上述效用公式来计算每个决策因素的分数。每个行动的分数是通过组合决策因素来计算的。攻击行动完全基于攻击欲望曲线。治疗决策基于健康欲望曲线乘以威胁曲线。这减轻了智能体可能过早治疗的情况，并根据玩家的最大伤害改变行为。值得注意的是，我们最初将其设置为玩家可以造成的平均伤害，因为这更符合预期效用的精神，但这并没有产生足够严格的结果。这是一个很好的例子，说明调整效用曲线和公式更多的是艺术而不是科学。逃跑的决策与治疗的决策方式相同；它将逃跑的欲望与威胁相乘。
9.9 结论
效用理论是一种非常强大的方法，可以让您的人工智能获得丰富、逼真的行为，并已被用于几乎所有类型的无数游戏中。它可以非常快，特别是如果您选择简单的效用函数，并且它的扩展性非常好。这个系统的一个巨大吸引力是，您可以通过一些简单的值和一些权重来添加个性，从而获得大量的涌现行为。通过以有意义的方式组合决策因素，您可以从这些原子组件构建决策，以提供非常深入的人工智能。
在本文中，我们对效用理论及其在游戏中的应用进行了快速浏览。我们向您展示了决策制定的一些基本原则，并深入探讨了其背后的数学原理。通过本文中的工具和一些工作，您可以构建一个强大的、涌现的人工智能系统。
