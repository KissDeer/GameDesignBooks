# 七杠三、人工智能通用图形处理器（GPGPU）简介



作者：Conan Bourke 和 Tomasz Bednarz

## 45.1 引言



在过去十年中，计算机硬件取得了长足的进步。单核、双核、四核，如今甚至有了成百上千个核心！计算机的运算能力已经从 CPU 转移到了 GPU，新的应用程序接口（API）允许程序员控制这些芯片进行除图形处理之外的更多操作。



随着硬件的每一次进步，游戏系统获得了越来越多的处理器能力，从而能够提供比以往想象中更加复杂和精细的体验。对于人工智能来说，这意味着能够创建更逼真的智能体，它们可以与彼此、玩家以及周围环境进行更复杂的交互。



硬件方面最重要的进步之一是 GPU 从纯粹的渲染处理器转变为在一定限制内能够进行任何我们所需计算的通用浮点处理器。两大硬件供应商 AMD 和 NVIDIA 生产的 GPU 通常都有 512 个以上的处理器，最新的消费级型号甚至提供了大约 3000 个处理器，每个处理器都能够并行处理数据。这是巨大的处理能力，而且我们不必将其全部用于高端图形处理。甚至 Xbox 360 的 GPU 也能够进行基本的通用处理，索尼的 PS3 也有类似 GPU 的架构，具备相同的处理能力。



这些处理器的一个常见缺点是 CPU 和 GPU 之间的延迟和带宽问题，但在这方面已经取得了一些进展，特别是 AMD 在将 GPU 的线性计算模型与 CPU 的通用处理模型相结合方面，推出了加速处理单元（APU），大大减少了延迟，并提供了其他优势，本章后面将对此进行讨论。

## 45.2 GPGPU 的历史



通用图形处理器（GPGPU）是指使用 GPU 进行除渲染之外的计算 [Harris 02]。随着早期着色器模型的引入，程序员能够修改 GPU 处理的方式和内容，不再依赖 OpenGL 或 Direct3D 的固定功能管线，也不必非得渲染出图像以供查看。相反，纹理可以用作数据缓冲区，在像素片段内进行访问和处理，结果可以绘制到输出纹理上。这种方法的缺点是缓冲区要么是只读的，要么是只写的。此外，元素独立性的限制以及需要在图形算法和渲染管线的背景下表示问题，使得这种技术使用起来很麻烦。尽管如此，着色器已经从简单的汇编语言程序发展成了几种新的类似 C 的语言，硬件供应商也开始认可不断发展的 GPGPU 领域。



人们致力于在渲染管线之外展示 GPU 的能力，因此创建了一些 API，使程序员能够利用 GPU 的能力，而不必将其仅仅视为基于图形的设备，并且这些 API 提供了具有读写修改访问权限和额外数学功能的缓冲区。



2007 年 2 月，NVIDIA 推出了计算统一设备架构（CUDA）API，与 G80 系列 GPU 一起极大地推动了 GPGPU 领域的发展。微软在 DirectX11 中发布了 DirectCompute API。它与 CUDA 有许多相似的理念和方法，但具有能够利用 DirectX 现有资源的优势，便于在 DirectCompute 和 Direct3D 之间共享数据以进行可视化。DirectCompute 也可在 DirectX10 硬件上使用，但与 DirectX 的其他部分一样，它只能在基于 Windows 的系统上运行。



一个竞争标准 OpenCL（开放计算语言）于 2007 年首次发布。OpenCL 使开发者能够通过一个基于现代 C 语言的单一编程接口，轻松编写在异构架构（如多核 CPU 和 GPU，甚至索尼的 PS3）上高效运行的跨平台应用程序。OpenCL 的规范由 Khronos 集团管理，他们还提供了一组 C++ 绑定，我们在本文中使用了这些绑定，这些绑定大大简化了主机 API 的设置和代码开发的速度。

## 45.3 OpenCL



OpenCL 程序被编写为 “内核”，这些函数在单个处理单元（计算单元）上并行执行，并且相互独立工作。内核代码与 C 代码非常相似，并支持许多内置的数学函数。



当主机程序运行时，我们需要执行以下步骤来使用 OpenCL：



1. OpenCL 会枚举系统中可用的平台和计算设备，即所有的 CPU 和 GPU。在每个设备内部有一个或多个计算单元，在这些计算单元内部有一个或多个处理单元来进行实际的计算。设备（CPU 或 GPU）能够同时执行的独立指令数量与处理单元的数量相对应。因此，一个具有 8 个单元的 CPU 仍然被视为一个单一设备，一个具有

2. 448 个单元的 GPU 也是如此。
   \2. 选择最强大的设备（通常是 GPU），并为进一步的操作设置其上下文。
   \3. 配置主机应用程序和 OpenCL 之间的数据共享。
   \4. 使用内核代码和 OpenCL 上下文为设备构建 OpenCL 程序，然后从编译后的内核代码中提取内核对象。
   \5. OpenCL 使用命令队列来控制内核执行的同步。向内核读写数据以及对内存对象的操作也由命令队列执行。
   \6. 调用内核，使其在所有处理单元上执行。并行线程共享内存并使用屏障进行同步。最后，将工作项的输出读回主机内存以供进一步操作。

   ## 45.4 简单的群体行为模拟和 OpenCL

   

   我们将简要介绍如何将经典的 AI 算法转换为使用 OpenCL 在 GPU 上运行。Craig Reynolds [Reynolds 87] 引入了用于控制自主移动智能体的转向行为概念，以及模拟人群和鸟群的群体行为和 boids 概念。许多即时战略游戏利用群体行为效果很好，例如 Relic Entertainment 的《家园》系列就是一个例子，但这些游戏通常在智能体数量上有限制。将此算法转换到 GPU 上运行，我们可以轻松地将智能体数量增加到数千个。

   

   我们将在 CPU 和 GPU 上实现一种简单的群体行为模拟的暴力方法，以展示在不进行任何分区的情况下，仅仅切换到 GPU 就能轻松获得的性能提升，这利用了 GPU 的大规模并行架构。类似的工作已经在 PS3 的 Cell 架构上完成 [Reynolds 06]，采用了简单的空间分区方案。清单 45.1 给出了一个基本的群体行为算法的伪代码，该算法使用分离、凝聚和对齐的加权和优先级力，还包括一个随机漫步行为来使智能体随机化。优先级依次为随机漫步、分离、凝聚，最后是对齐。在将速度应用于位置之前，必须更新所有速度，否则初始智能体会错误地影响后续智能体。

   

   在 CPU 上实现此算法很简单。通常，一个智能体由一个包含相关智能体信息（如位置、速度、随机漫步目标等）的对象组成。然后会对智能体数组进行两次循环：第一次更新每个智能体的力和速度，第二次应用新的速度。

   

   将此算法转换到 GPU 时，有一些事项需要考虑，但 CPU 代码到 OpenCL 代码的转换很直接。如上述伪代码所示，每个智能体都要计算所有邻居，这具有 的复杂度。在 CPU 上，这是通过对所有智能体进行双重循环来实现的。在 GPU 上，我们可以并行化外层循环，并对每个工作项（每个智能体）顺序执行内层循环交互，大大减少了处理时间。

   

   可以实施空间分区技术来提高性能，但需要注意的是，GPU 以非常线性的方式工作，非常适合处理数据数组，就像它在图形处理中处理顶点数组一样。在复杂的空间分区方案（如八叉树）的情况下，GPU 在尝试访问非线性内存时会遇到困难。Craig Reynolds 为 PS3 提供的解决方案是使用一个简单的三维桶状网格来存储相邻智能体 [Reynolds 06]。这允许线性地处理桶，智能体只需要对直接相邻的桶具有读访问权限。在本文中，我们展示了从 CPU 到 GPU 的简单转换，没有进行这种优化，以展示转换到 GPGPU 处理的即时收益。

   

   转换到 GPGPU 时的第一步是将数据分解为连续的数组。GPU 可以处理多达三维的数组，但在我们的示例中，我们将为智能体的每个元素（即位置、速度等）将智能体分解为一维数组。

   

   还值得注意的是，在 OpenCL 术语中，有两种类型的内存：本地内存和全局内存。区别在于全局内存可以被任何核心访问，而本地内存对于一个进程是唯一的，因此访问速度更快。可以将其想象成内存（RAM）和 CPU 的缓存。

   ## 45.5 OpenCL 设置

   

   使用 C++ 主机绑定初始化计算设备很简单。首先，必须枚举主机平台以访问底层计算设备。然后从一个平台创建一个上下文（在本例中，我们使用 CL_DEVICE_TYPE_GPU 专门初始化上下文以使用 GPU）以及一个命令队列，以便通过上下文执行计算内核和排队内存传输。详情请参阅清单 45.2。

   

   OpenCL 有两种类型的内存对象：缓冲区和图像。缓冲区使用单指令多数据（SIMD）处理模型包含标准的 4D 浮点向量，而图像是根据纹理元素定义的。在本文中，选择缓冲区更适合表示彼此相邻的智能体。

   

   缓冲区可以初始化为只读、只写或读写，如清单 45.3 所示。创建缓冲区是为了容纳模拟将使用的最大智能体数量，不过如果需要，我们也可以处理更少的智能体数量。除了智能体数据外，我们还向内核发送群体行为算法的参数，以及一个指定自上一帧以来经过时间的时间值，以保持速度的一致性。

   

   为了创建计算内核，我们需要将内核代码编译成 CL 程序，然后提取计算内核。在我们的示例中，内核代码位于一个单独的文件 program.cl 中，加载该文件以创建程序，如清单 45.4 所示。

   

   清单 45.5 显示了我们示例内核的一部分，由于其与 CPU 实现几乎相同，因此省略了主体部分。然而，值得注意的是内核的最后一部分与屏障有关。在 CPU 上，我们在计算完所有智能体的力后循环两次将力应用于所有智能体。在核函数中，我们可以通过设置一个屏障来实现这一点，这会使所有执行的线程在此处等待，直到所有线程都跟上。在核函数中，我们可以通过调用 get_global_id (0) 并根据缓冲区维度使用 0、1 或 2 来访问输入缓冲区的当前索引。

   

   一旦构建了内核，就需要显式地将内核参数传递给 OpenCL，如清单 45.6 所示。不是在执行内核时传递参数，而是必须将参数预先加载到相应的参数索引中。

   

   一旦一切都初始化和构建完成，我们就可以将内核排队进行计算。内核不会立即执行，而是排队等待处理。必须使用等于要处理的元素数量的全局工作大小来启动内核。我们也可以指定数组范围的偏移量，但可以指定一个空范围从数组开头开始。请参阅清单 45.7。

   ## 45.6 系统间共享 GPU 处理

   

   开发人员在使用 GPGPU 时，尤其是游戏开发者，最初可能会担心图形处理的时间会被占用。如今许多高端游戏使用 GPGPU 进行图形预处理和后处理，以及使用诸如 NVIDIA 的 PhysX 等 API 进行物理模拟。再加入人工智能将会减少这些其他系统可用的处理时间。这是一个无法避免的问题。然而，GPU 的处理能力已经有了巨大的飞跃，从 NVIDIA 500 系列的数百个核心增加到 600 系列的数千个核心。随着时间的推移，更多的处理能力将可用于更多的系统，开发者将开始发现除了图形、物理和人工智能之外的其他有趣用途。

   

   同时，至少对于 OpenCL 来说，存在互操作性 API，允许在 OpenGL 和 Direct3D 之间共享 OpenCL 缓冲区，减少了不断将信息传输到 GPU 再传输回 CPU 的需求。人工智能智能体的位置缓冲区既可以用于 GPU 上的群体行为计算，例如用于渲染智能体的硬件实例化的渲染缓冲区，而无需将数据先返回 CPU 再传输回 GPU。

   ## 45.7 结果

   

   图 45.1 显示了使用我们的示例群体行为算法的暴力实现处理智能体的性能（以毫秒为单位，越低越好）。很明显，随着智能体数量的增加，GPU 提供了巨大的性能提升，而且将算法转换为 OpenCL 所需的工作量极小。然而，在智能体数量较少时，由于缓冲区传输，GPU 比 CPU 运行得慢，如图 45.2 所示。我们还测试了一款用于计算和研究的 GPU，以展示此类设备的优化带宽和延迟，其计算速度比消费级 GPU 更快，同时也让我们对未来消费级性能有了一定的了解。

   ## 45.8 结论

   

   正如我们的示例所示，对于需要大量智能体的游戏，如使用数千个实体的即时战略游戏（而不是当前大多数即时战略游戏中的标准几十到几百个），我们可以轻松地使用 GPGPU 计算。我们仍然难以将智能体的决策部分转移到 GPU 上，但诸如运动甚至避障等元素可以从 CPU 转移。从其他系统（如图形处理）占用处理时间将是另一个问题，但可以通过各种互操作性 API 得到一定程度的缓解。

   

   还有其他人工智能的 GPGPU 应用示例，一些团队已经在神经网络和路径查找方面进行了相关工作，经典的康威生命游戏也可以很容易地在 GPU 上实现 [Rumpf 10]。GPGPU 在人工智能处理类型方面的主要限制是大多数人工智能决策的分支性质。

   

   APU 可以使我们将决策技术与 GPGPU 技术紧密结合，但消费者对这类设备的接受程度将决定这种人工智能风格是否会在游戏中更频繁地出现。

   

   就目前而言，GPGPU 是大规模模拟的可行选择，根据 Valve 的硬件调查，在撰写本文时，最常见的消费级 GPU 是 NVIDIA GTX 560，它有 336 个处理器，足以满足我们的人工智能需求。